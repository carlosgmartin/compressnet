$ python3 train.py models/model
iteration 0 loss: 2.061863660812378
iteration 1 loss: 2.121993064880371
iteration 2 loss: 2.007594108581543
iteration 3 loss: 2.014979362487793
iteration 4 loss: 2.111457586288452
iteration 5 loss: 1.9938246011734009
iteration 6 loss: 2.009352207183838
iteration 7 loss: 2.139270067214966
iteration 8 loss: 2.112551212310791
iteration 9 loss: 2.3023386001586914
iteration 10 loss: 1.9417732954025269
iteration 11 loss: 1.995069980621338
iteration 12 loss: 2.0624635219573975
iteration 13 loss: 2.091953754425049
iteration 14 loss: 2.1084909439086914
iteration 15 loss: 2.0751261711120605
iteration 16 loss: 2.0633716583251953
iteration 17 loss: 1.9762135744094849
iteration 18 loss: 2.0764784812927246
iteration 19 loss: 2.0679028034210205
iteration 20 loss: 1.9908199310302734
iteration 21 loss: 2.0952348709106445
iteration 22 loss: 2.146544933319092
iteration 23 loss: 2.0216121673583984
iteration 24 loss: 1.9771578311920166
iteration 25 loss: 2.1094770431518555
iteration 26 loss: 2.0119833946228027
iteration 27 loss: 1.9893512725830078
iteration 28 loss: 1.9632289409637451
iteration 29 loss: 2.2426350116729736
iteration 30 loss: 1.9975076913833618
iteration 31 loss: 1.952675223350525
iteration 32 loss: 1.919228196144104
iteration 33 loss: 2.0152230262756348
iteration 34 loss: 2.0557539463043213
iteration 35 loss: 2.0462658405303955
iteration 36 loss: 2.076955556869507
iteration 37 loss: 1.8929660320281982
iteration 38 loss: 1.9339542388916016
iteration 39 loss: 2.0165534019470215
iteration 40 loss: 1.9783624410629272
iteration 41 loss: 1.9169056415557861
iteration 42 loss: 1.948177695274353
iteration 43 loss: 2.125789165496826
iteration 44 loss: 1.9355350732803345
iteration 45 loss: 2.0550003051757812
iteration 46 loss: 2.0993106365203857
iteration 47 loss: 1.9891737699508667
iteration 48 loss: 1.9760159254074097
iteration 49 loss: 1.9773255586624146
iteration 50 loss: 1.9866522550582886
iteration 51 loss: 1.9113874435424805
iteration 52 loss: 1.921180009841919
iteration 53 loss: 1.8232382535934448
iteration 54 loss: 2.0300991535186768
iteration 55 loss: 1.9042177200317383
iteration 56 loss: 1.9734729528427124
iteration 57 loss: 2.0142006874084473
iteration 58 loss: 1.9444074630737305
iteration 59 loss: 1.9525914192199707
iteration 60 loss: 1.8678150177001953
iteration 61 loss: 2.0142712593078613
iteration 62 loss: 2.099872589111328
iteration 63 loss: 2.078256130218506
iteration 64 loss: 1.9739189147949219
iteration 65 loss: 1.9852296113967896
iteration 66 loss: 2.177031993865967
iteration 67 loss: 2.0260214805603027
iteration 68 loss: 2.082871437072754
iteration 69 loss: 1.9943960905075073
iteration 70 loss: 2.0745363235473633
iteration 71 loss: 1.9603166580200195
iteration 72 loss: 2.079383373260498
iteration 73 loss: 2.052170991897583
iteration 74 loss: 1.893658995628357
iteration 75 loss: 2.082744598388672
iteration 76 loss: 2.015062093734741
iteration 77 loss: 2.054827928543091
iteration 78 loss: 1.9683611392974854
iteration 79 loss: 2.1201882362365723
iteration 80 loss: 2.113487482070923
iteration 81 loss: 1.9300769567489624
iteration 82 loss: 1.9333581924438477
iteration 83 loss: 1.9875433444976807
iteration 84 loss: 1.964469313621521
iteration 85 loss: 1.9563977718353271
iteration 86 loss: 1.976237416267395
iteration 87 loss: 1.9161573648452759
iteration 88 loss: 2.021489143371582
iteration 89 loss: 2.0052413940429688
iteration 90 loss: 2.059394121170044
iteration 91 loss: 1.9843493700027466
iteration 92 loss: 2.078322649002075
iteration 93 loss: 2.148085594177246
iteration 94 loss: 1.996388554573059
iteration 95 loss: 1.935080885887146
iteration 96 loss: 1.9228307008743286
iteration 97 loss: 1.9965163469314575
iteration 98 loss: 2.1324563026428223
iteration 99 loss: 2.0320701599121094
iteration 100 loss: 1.9946364164352417
iteration 101 loss: 2.0623443126678467
iteration 102 loss: 2.0407958030700684
iteration 103 loss: 2.0553171634674072
iteration 104 loss: 1.9780999422073364
iteration 105 loss: 2.091547727584839
iteration 106 loss: 1.957217812538147
iteration 107 loss: 1.8655486106872559
iteration 108 loss: 1.9181833267211914
iteration 109 loss: 1.98581862449646
iteration 110 loss: 1.849765658378601
iteration 111 loss: 2.03621244430542
iteration 112 loss: 1.9684677124023438
iteration 113 loss: 1.9238845109939575
iteration 114 loss: 2.0288963317871094
iteration 115 loss: 1.9001556634902954
iteration 116 loss: 2.000532865524292
iteration 117 loss: 2.025562047958374
iteration 118 loss: 2.006045341491699
iteration 119 loss: 2.0379292964935303
iteration 120 loss: 1.886228084564209
iteration 121 loss: 1.895820140838623
iteration 122 loss: 2.035695791244507
iteration 123 loss: 1.9325414896011353
iteration 124 loss: 1.9873335361480713
iteration 125 loss: 1.9463403224945068
iteration 126 loss: 2.0770785808563232
iteration 127 loss: 2.116427421569824
iteration 128 loss: 2.0052473545074463
iteration 129 loss: 1.9606101512908936
iteration 130 loss: 2.0690295696258545
iteration 131 loss: 1.9598124027252197
iteration 132 loss: 2.036759376525879
iteration 133 loss: 1.8827321529388428
iteration 134 loss: 1.923783779144287
iteration 135 loss: 1.980533242225647
iteration 136 loss: 1.9990712404251099
iteration 137 loss: 1.9798383712768555
iteration 138 loss: 1.953049659729004
iteration 139 loss: 1.9964135885238647
iteration 140 loss: 2.0781664848327637
iteration 141 loss: 1.9150580167770386
iteration 142 loss: 1.9232319593429565
iteration 143 loss: 1.9984266757965088
iteration 144 loss: 2.0421218872070312
iteration 145 loss: 1.9548475742340088
iteration 146 loss: 2.0451152324676514
iteration 147 loss: 1.9917664527893066
iteration 148 loss: 1.8699429035186768
iteration 149 loss: 2.013601064682007
iteration 150 loss: 1.8601641654968262
iteration 151 loss: 2.0657742023468018
iteration 152 loss: 1.998360276222229
iteration 153 loss: 1.8914583921432495
iteration 154 loss: 1.8899178504943848
iteration 155 loss: 1.9474539756774902
iteration 156 loss: 2.081097364425659
iteration 157 loss: 1.949734091758728
iteration 158 loss: 2.029263734817505
iteration 159 loss: 2.0093016624450684
iteration 160 loss: 1.9773037433624268
iteration 161 loss: 1.938441276550293
iteration 162 loss: 1.9372693300247192
iteration 163 loss: 1.8479154109954834
iteration 164 loss: 1.8795552253723145
iteration 165 loss: 1.9227055311203003
iteration 166 loss: 1.9412513971328735
iteration 167 loss: 1.950941801071167
iteration 168 loss: 1.8557989597320557
iteration 169 loss: 2.0286383628845215
iteration 170 loss: 1.968485951423645
iteration 171 loss: 2.065098285675049
iteration 172 loss: 1.8500945568084717
iteration 173 loss: 1.9802792072296143
iteration 174 loss: 1.9908844232559204
iteration 175 loss: 1.941320776939392
iteration 176 loss: 1.9222456216812134
iteration 177 loss: 2.023413896560669
iteration 178 loss: 2.025036096572876
iteration 179 loss: 1.9057154655456543
iteration 180 loss: 2.0577797889709473
iteration 181 loss: 2.0232083797454834
iteration 182 loss: 1.9772634506225586
iteration 183 loss: 2.1012120246887207
iteration 184 loss: 2.0571091175079346
iteration 185 loss: 1.9187191724777222
iteration 186 loss: 2.044246196746826
iteration 187 loss: 1.8847702741622925
iteration 188 loss: 2.005694627761841
iteration 189 loss: 1.9551591873168945
iteration 190 loss: 1.9309197664260864
iteration 191 loss: 1.945367693901062
iteration 192 loss: 1.9139682054519653
iteration 193 loss: 1.8837658166885376
iteration 194 loss: 1.8782533407211304
iteration 195 loss: 2.0045034885406494
iteration 196 loss: 2.0033226013183594
iteration 197 loss: 2.0481770038604736
iteration 198 loss: 1.9137084484100342
iteration 199 loss: 1.9498281478881836
iteration 200 loss: 1.902207612991333
iteration 201 loss: 1.8064892292022705
iteration 202 loss: 1.892061471939087
iteration 203 loss: 1.9783204793930054
iteration 204 loss: 2.027207136154175
iteration 205 loss: 1.8481132984161377
iteration 206 loss: 1.9561482667922974
iteration 207 loss: 1.8648364543914795
iteration 208 loss: 2.0531764030456543
iteration 209 loss: 1.971294641494751
iteration 210 loss: 1.967874526977539
iteration 211 loss: 1.9503978490829468
iteration 212 loss: 1.8724836111068726
iteration 213 loss: 1.988574743270874
iteration 214 loss: 1.9050815105438232
iteration 215 loss: 1.9740089178085327
iteration 216 loss: 1.9541032314300537
iteration 217 loss: 1.9660624265670776
iteration 218 loss: 2.0232207775115967
iteration 219 loss: 1.8974355459213257
iteration 220 loss: 1.9333986043930054
iteration 221 loss: 1.8684639930725098
iteration 222 loss: 1.973489761352539
iteration 223 loss: 2.0218112468719482
iteration 224 loss: 1.9948985576629639
iteration 225 loss: 1.9348691701889038
iteration 226 loss: 2.1052887439727783
iteration 227 loss: 1.979900598526001
iteration 228 loss: 1.8386083841323853
iteration 229 loss: 2.172882080078125
iteration 230 loss: 2.029421806335449
iteration 231 loss: 1.9475407600402832
iteration 232 loss: 2.002626419067383
iteration 233 loss: 1.9761402606964111
iteration 234 loss: 1.9186270236968994
iteration 235 loss: 1.9148428440093994
iteration 236 loss: 2.0126912593841553
iteration 237 loss: 1.8722052574157715
iteration 238 loss: 1.9021457433700562
iteration 239 loss: 2.002525568008423
iteration 240 loss: 1.9719672203063965
iteration 241 loss: 1.9389445781707764
iteration 242 loss: 2.0581905841827393
iteration 243 loss: 1.824273943901062
iteration 244 loss: 1.9615458250045776
iteration 245 loss: 1.969951868057251
iteration 246 loss: 2.055598497390747
iteration 247 loss: 1.9216718673706055
iteration 248 loss: 1.8794571161270142
iteration 249 loss: 1.954479694366455
iteration 250 loss: 1.9887653589248657
iteration 251 loss: 1.851679801940918
iteration 252 loss: 1.8738521337509155
iteration 253 loss: 1.8639628887176514
iteration 254 loss: 1.9865496158599854
iteration 255 loss: 1.858875036239624
iteration 256 loss: 1.9526865482330322
iteration 257 loss: 1.9926131963729858
iteration 258 loss: 1.8329638242721558
iteration 259 loss: 2.006601572036743
iteration 260 loss: 1.9964265823364258
iteration 261 loss: 1.9562088251113892
iteration 262 loss: 1.9690191745758057
iteration 263 loss: 1.9438692331314087
iteration 264 loss: 1.9488519430160522
iteration 265 loss: 1.8827980756759644
iteration 266 loss: 2.005021095275879
iteration 267 loss: 2.031332492828369
iteration 268 loss: 1.9903786182403564
iteration 269 loss: 1.8374639749526978
iteration 270 loss: 1.8993847370147705
iteration 271 loss: 1.8257806301116943
iteration 272 loss: 1.9131778478622437
iteration 273 loss: 1.9257663488388062
iteration 274 loss: 1.890024185180664
iteration 275 loss: 1.8270924091339111
iteration 276 loss: 1.981812834739685
iteration 277 loss: 1.902710199356079
iteration 278 loss: 1.8929722309112549
iteration 279 loss: 1.940173864364624
iteration 280 loss: 1.8525902032852173
iteration 281 loss: 2.0120303630828857
iteration 282 loss: 1.9410098791122437
iteration 283 loss: 1.9612674713134766
iteration 284 loss: 1.8868701457977295
iteration 285 loss: 2.0217297077178955
iteration 286 loss: 1.855387806892395
iteration 287 loss: 1.977856993675232
iteration 288 loss: 1.9975345134735107
iteration 289 loss: 1.7959558963775635
iteration 290 loss: 1.8925753831863403
iteration 291 loss: 1.9016227722167969
iteration 292 loss: 1.838523030281067
iteration 293 loss: 1.8803036212921143
iteration 294 loss: 1.9470679759979248
iteration 295 loss: 1.871957540512085
iteration 296 loss: 1.9591355323791504
iteration 297 loss: 1.8185480833053589
iteration 298 loss: 1.8505496978759766
iteration 299 loss: 1.9833158254623413
iteration 300 loss: 2.05542254447937
iteration 301 loss: 1.917946696281433
iteration 302 loss: 1.8522522449493408
iteration 303 loss: 1.8652594089508057
iteration 304 loss: 1.9791367053985596
iteration 305 loss: 1.8618595600128174
iteration 306 loss: 1.8038934469223022
iteration 307 loss: 1.876977801322937
iteration 308 loss: 1.8537793159484863
iteration 309 loss: 1.8427718877792358
iteration 310 loss: 2.0420565605163574
iteration 311 loss: 1.816652536392212
iteration 312 loss: 1.9654550552368164
iteration 313 loss: 2.0386362075805664
iteration 314 loss: 1.797221064567566
iteration 315 loss: 1.729005217552185
iteration 316 loss: 1.9706965684890747
iteration 317 loss: 1.9230506420135498
iteration 318 loss: 1.9147100448608398
iteration 319 loss: 1.890009880065918
iteration 320 loss: 1.8361743688583374
iteration 321 loss: 1.810052514076233
iteration 322 loss: 1.919599175453186
iteration 323 loss: 1.9380606412887573
iteration 324 loss: 1.9155607223510742
iteration 325 loss: 1.9089055061340332
iteration 326 loss: 1.8256903886795044
iteration 327 loss: 1.848702073097229
iteration 328 loss: 1.8017743825912476
iteration 329 loss: 1.8696508407592773
iteration 330 loss: 1.8752728700637817
iteration 331 loss: 1.862420916557312
iteration 332 loss: 1.8332535028457642
iteration 333 loss: 1.7426657676696777
iteration 334 loss: 1.9447559118270874
iteration 335 loss: 2.054475784301758
iteration 336 loss: 1.8624188899993896
iteration 337 loss: 1.7844892740249634
iteration 338 loss: 1.9205728769302368
iteration 339 loss: 1.8365873098373413
iteration 340 loss: 1.9258463382720947
iteration 341 loss: 1.904486060142517
iteration 342 loss: 1.8373690843582153
iteration 343 loss: 1.870861530303955
iteration 344 loss: 1.8202564716339111
iteration 345 loss: 1.7796313762664795
iteration 346 loss: 1.768697738647461
iteration 347 loss: 1.8784412145614624
iteration 348 loss: 1.9389612674713135
iteration 349 loss: 1.8790489435195923
iteration 350 loss: 1.8495206832885742
iteration 351 loss: 1.796454668045044
iteration 352 loss: 1.808741569519043
iteration 353 loss: 1.954429268836975
iteration 354 loss: 1.923517107963562
iteration 355 loss: 1.7859704494476318
iteration 356 loss: 1.9445699453353882
iteration 357 loss: 1.8313757181167603
iteration 358 loss: 1.8874584436416626
iteration 359 loss: 1.8948333263397217
iteration 360 loss: 1.9432379007339478
iteration 361 loss: 1.9730682373046875
iteration 362 loss: 1.871251106262207
iteration 363 loss: 1.8966959714889526
iteration 364 loss: 2.019653081893921
iteration 365 loss: 1.883100986480713
iteration 366 loss: 1.8585124015808105
iteration 367 loss: 1.9549280405044556
iteration 368 loss: 1.8071424961090088
iteration 369 loss: 1.847263216972351
iteration 370 loss: 1.8573392629623413
iteration 371 loss: 1.7538954019546509
iteration 372 loss: 1.775546908378601
iteration 373 loss: 1.847141146659851
iteration 374 loss: 1.8685871362686157
iteration 375 loss: 1.7570408582687378
iteration 376 loss: 1.8499997854232788
iteration 377 loss: 1.849056363105774
iteration 378 loss: 1.7036212682724
iteration 379 loss: 1.8711844682693481
iteration 380 loss: 1.9471222162246704
iteration 381 loss: 1.8992201089859009
iteration 382 loss: 1.8868447542190552
iteration 383 loss: 1.9092864990234375
iteration 384 loss: 1.9067620038986206
iteration 385 loss: 1.8931971788406372
iteration 386 loss: 1.7184828519821167
iteration 387 loss: 1.8309227228164673
iteration 388 loss: 1.7461966276168823
iteration 389 loss: 1.8827403783798218
iteration 390 loss: 1.8407702445983887
iteration 391 loss: 1.877310872077942
iteration 392 loss: 1.8389462232589722
iteration 393 loss: 1.8904318809509277
iteration 394 loss: 1.8966000080108643
iteration 395 loss: 1.828693151473999
iteration 396 loss: 1.870537281036377
iteration 397 loss: 1.8764861822128296
iteration 398 loss: 1.8160265684127808
iteration 399 loss: 1.8551201820373535
iteration 400 loss: 1.8089967966079712
iteration 401 loss: 1.7546122074127197
iteration 402 loss: 1.7940853834152222
iteration 403 loss: 1.8564523458480835
iteration 404 loss: 1.7832788228988647
iteration 405 loss: 1.9497182369232178
iteration 406 loss: 1.8828660249710083
iteration 407 loss: 1.742897391319275
iteration 408 loss: 1.8864394426345825
iteration 409 loss: 1.8733934164047241
iteration 410 loss: 1.8492565155029297
iteration 411 loss: 1.8898069858551025
iteration 412 loss: 1.756238579750061
iteration 413 loss: 1.7539401054382324
iteration 414 loss: 1.8902050256729126
iteration 415 loss: 1.8080357313156128
iteration 416 loss: 1.8953005075454712
iteration 417 loss: 1.8119717836380005
iteration 418 loss: 1.8214813470840454
iteration 419 loss: 1.844550609588623
iteration 420 loss: 1.754649043083191
iteration 421 loss: 1.791944980621338
iteration 422 loss: 1.7340333461761475
iteration 423 loss: 1.974220871925354
iteration 424 loss: 1.9646382331848145
iteration 425 loss: 1.7988044023513794
iteration 426 loss: 1.8036524057388306
iteration 427 loss: 1.8438373804092407
iteration 428 loss: 1.9191617965698242
iteration 429 loss: 1.8711718320846558
iteration 430 loss: 1.788297414779663
iteration 431 loss: 1.8241240978240967
iteration 432 loss: 1.9260152578353882
iteration 433 loss: 1.750392198562622
iteration 434 loss: 1.802797555923462
iteration 435 loss: 1.8632951974868774
iteration 436 loss: 1.838894248008728
iteration 437 loss: 1.732541799545288
iteration 438 loss: 1.7445425987243652
iteration 439 loss: 1.7557940483093262
iteration 440 loss: 2.0666887760162354
iteration 441 loss: 1.8240033388137817
iteration 442 loss: 1.7651140689849854
iteration 443 loss: 1.724557638168335
iteration 444 loss: 1.8608458042144775
iteration 445 loss: 1.6153758764266968
iteration 446 loss: 1.880954384803772
iteration 447 loss: 1.834825038909912
iteration 448 loss: 1.887325406074524
iteration 449 loss: 1.6443233489990234
iteration 450 loss: 1.8687090873718262
iteration 451 loss: 1.871847152709961
iteration 452 loss: 1.880061149597168
iteration 453 loss: 1.833863377571106
iteration 454 loss: 1.7994990348815918
iteration 455 loss: 1.8836474418640137
iteration 456 loss: 1.8174020051956177
iteration 457 loss: 1.73838210105896
iteration 458 loss: 1.895957112312317
iteration 459 loss: 1.9216041564941406
iteration 460 loss: 1.6885411739349365
iteration 461 loss: 1.9210636615753174
iteration 462 loss: 1.89791738986969
iteration 463 loss: 1.8711791038513184
iteration 464 loss: 1.8454846143722534
iteration 465 loss: 1.8422844409942627
iteration 466 loss: 1.8002103567123413
iteration 467 loss: 1.8912746906280518
iteration 468 loss: 1.790365219116211
iteration 469 loss: 1.737048625946045
iteration 470 loss: 1.8351393938064575
iteration 471 loss: 1.8410707712173462
iteration 472 loss: 1.8167177438735962
iteration 473 loss: 1.7642507553100586
iteration 474 loss: 1.7691015005111694
iteration 475 loss: 1.6951558589935303
iteration 476 loss: 1.8350841999053955
iteration 477 loss: 1.71616530418396
iteration 478 loss: 1.8144187927246094
iteration 479 loss: 1.9014707803726196
iteration 480 loss: 1.875022053718567
iteration 481 loss: 1.7512233257293701
iteration 482 loss: 1.8820645809173584
iteration 483 loss: 1.7518264055252075
iteration 484 loss: 1.7570253610610962
iteration 485 loss: 1.8714832067489624
iteration 486 loss: 1.9297353029251099
iteration 487 loss: 1.7927680015563965
iteration 488 loss: 1.8236016035079956
iteration 489 loss: 1.8549270629882812
iteration 490 loss: 1.834707498550415
iteration 491 loss: 1.794999599456787
iteration 492 loss: 1.8256713151931763
iteration 493 loss: 1.802505612373352
iteration 494 loss: 1.9311809539794922
iteration 495 loss: 1.819865345954895
iteration 496 loss: 1.9535409212112427
iteration 497 loss: 1.6896438598632812
iteration 498 loss: 1.8153352737426758
iteration 499 loss: 1.7773388624191284
iteration 500 loss: 1.7682061195373535
iteration 501 loss: 1.8042936325073242
iteration 502 loss: 1.682088017463684
iteration 503 loss: 1.7676922082901
iteration 504 loss: 1.6041125059127808
iteration 505 loss: 1.7742490768432617
iteration 506 loss: 1.810482144355774
iteration 507 loss: 1.85227632522583
iteration 508 loss: 1.7769289016723633
iteration 509 loss: 1.6923028230667114
iteration 510 loss: 1.7917438745498657
iteration 511 loss: 1.8347108364105225
iteration 512 loss: 1.8409799337387085
iteration 513 loss: 1.7507621049880981
iteration 514 loss: 1.742310881614685
iteration 515 loss: 1.9055249691009521
iteration 516 loss: 1.7413955926895142
iteration 517 loss: 1.6223275661468506
iteration 518 loss: 1.720584511756897
iteration 519 loss: 1.7370415925979614
iteration 520 loss: 1.6924127340316772
iteration 521 loss: 1.866763949394226
iteration 522 loss: 1.7932795286178589
iteration 523 loss: 1.9012644290924072
iteration 524 loss: 1.9332809448242188
iteration 525 loss: 1.7113053798675537
iteration 526 loss: 1.7533503770828247
iteration 527 loss: 1.8486301898956299
iteration 528 loss: 1.7048242092132568
iteration 529 loss: 1.9082008600234985
iteration 530 loss: 1.801132082939148
iteration 531 loss: 1.7960401773452759
iteration 532 loss: 1.7408095598220825
iteration 533 loss: 1.8891510963439941
iteration 534 loss: 1.808095097541809
iteration 535 loss: 1.7922993898391724
iteration 536 loss: 1.9006147384643555
iteration 537 loss: 1.7800219058990479
iteration 538 loss: 1.7884103059768677
iteration 539 loss: 1.9011527299880981
iteration 540 loss: 1.7943921089172363
iteration 541 loss: 1.7682292461395264
iteration 542 loss: 1.731831669807434
iteration 543 loss: 1.8887990713119507
iteration 544 loss: 1.8585184812545776
iteration 545 loss: 1.7876825332641602
iteration 546 loss: 1.7254834175109863
iteration 547 loss: 1.901831030845642
iteration 548 loss: 1.8054395914077759
iteration 549 loss: 1.776033639907837
iteration 550 loss: 1.7210458517074585
iteration 551 loss: 1.9141991138458252
iteration 552 loss: 1.7093355655670166
iteration 553 loss: 1.845375657081604
iteration 554 loss: 1.6427353620529175
iteration 555 loss: 1.866500973701477
iteration 556 loss: 1.61118745803833
iteration 557 loss: 1.7622134685516357
iteration 558 loss: 1.7923792600631714
iteration 559 loss: 1.7079983949661255
iteration 560 loss: 1.752793550491333
iteration 561 loss: 1.6612122058868408
iteration 562 loss: 1.753039002418518
iteration 563 loss: 1.6516149044036865
iteration 564 loss: 1.6737172603607178
iteration 565 loss: 1.8542511463165283
iteration 566 loss: 1.8084839582443237
iteration 567 loss: 1.6576679944992065
iteration 568 loss: 1.7004070281982422
iteration 569 loss: 1.7627015113830566
iteration 570 loss: 1.696901798248291
iteration 571 loss: 1.6915881633758545
iteration 572 loss: 1.7401151657104492
iteration 573 loss: 1.6366034746170044
iteration 574 loss: 1.8237597942352295
iteration 575 loss: 1.8013836145401
iteration 576 loss: 1.9252406358718872
iteration 577 loss: 1.8935418128967285
iteration 578 loss: 1.780596137046814
iteration 579 loss: 1.8412058353424072
iteration 580 loss: 1.8495427370071411
iteration 581 loss: 1.760912537574768
iteration 582 loss: 1.8951345682144165
iteration 583 loss: 1.701016902923584
iteration 584 loss: 1.8142715692520142
iteration 585 loss: 1.7523044347763062
iteration 586 loss: 1.71171236038208
iteration 587 loss: 1.7327958345413208
iteration 588 loss: 1.8010896444320679
iteration 589 loss: 1.8268643617630005
iteration 590 loss: 1.6888741254806519
iteration 591 loss: 1.7093136310577393
iteration 592 loss: 1.8876675367355347
iteration 593 loss: 1.73026442527771
iteration 594 loss: 1.7802886962890625
iteration 595 loss: 1.8216347694396973
iteration 596 loss: 1.838316559791565
iteration 597 loss: 1.6747751235961914
iteration 598 loss: 1.7606815099716187
iteration 599 loss: 1.76976478099823
iteration 600 loss: 1.671486735343933
iteration 601 loss: 1.7316710948944092
iteration 602 loss: 1.6801881790161133
iteration 603 loss: 1.740304708480835
iteration 604 loss: 1.6178901195526123
iteration 605 loss: 1.6351630687713623
iteration 606 loss: 1.796741008758545
iteration 607 loss: 1.8776671886444092
iteration 608 loss: 1.735371708869934
iteration 609 loss: 1.8348214626312256
iteration 610 loss: 1.7487653493881226
iteration 611 loss: 1.9300358295440674
iteration 612 loss: 1.7490838766098022
iteration 613 loss: 1.7649601697921753
iteration 614 loss: 1.7230602502822876
iteration 615 loss: 1.6571539640426636
iteration 616 loss: 1.71549391746521
iteration 617 loss: 1.6791343688964844
iteration 618 loss: 1.6574432849884033
iteration 619 loss: 1.756582260131836
iteration 620 loss: 1.6263055801391602
iteration 621 loss: 1.7924106121063232
iteration 622 loss: 1.884860634803772
iteration 623 loss: 1.7289965152740479
iteration 624 loss: 1.636353850364685
iteration 625 loss: 1.7582721710205078
iteration 626 loss: 1.7897753715515137
iteration 627 loss: 1.697269320487976
iteration 628 loss: 1.6991413831710815
iteration 629 loss: 1.8692891597747803
iteration 630 loss: 1.595943808555603
iteration 631 loss: 1.6686581373214722
iteration 632 loss: 1.6936352252960205
iteration 633 loss: 1.8032017946243286
iteration 634 loss: 1.7453548908233643
iteration 635 loss: 1.7770342826843262
iteration 636 loss: 1.6256494522094727
iteration 637 loss: 1.7252708673477173
iteration 638 loss: 1.777976155281067
iteration 639 loss: 1.7604888677597046
iteration 640 loss: 1.6719210147857666
iteration 641 loss: 1.653931736946106
iteration 642 loss: 1.8547767400741577
iteration 643 loss: 1.6466777324676514
iteration 644 loss: 1.8276903629302979
iteration 645 loss: 1.6673719882965088
iteration 646 loss: 1.667702317237854
iteration 647 loss: 1.8345694541931152
iteration 648 loss: 1.6527421474456787
iteration 649 loss: 1.6920480728149414

